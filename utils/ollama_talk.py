# 在服务器端启动相应服务

import requests

'''
{'model': 'qwen2-7b-fp16', 'created_at': '2024-09-13T08:58:23.1539888Z', 'response': '你好！很高兴能与您交流。如果您有任何问题或需要帮助，请随时告诉我，我会尽力提供支持和解答。无论是关于技术、知识、日常建议还是只是闲聊，我都在这里等待您的提问。祝您今天一切顺利！', 'done': True, 'done_reason': 'stop', 'context': [151644, 872, 198, 36587, 108386, 151645, 198, 151644, 77091, 198, 108386, 6313, 112169, 26232, 57218, 87026, 101069, 1773, 106870, 110117, 86119, 57191, 85106, 100364, 37945, 102422, 106525, 3837, 105351, 110121, 99553, 100143, 33108, 106185, 1773, 102215, 101888, 99361, 5373, 100032, 5373, 101254, 101898, 99998, 100009, 100836, 100281, 3837, 35946, 102070, 99817, 104525, 101214, 107666, 1773, 100549, 87026, 100644, 101109, 102088, 6313], 'total_duration': 1392181000, 'load_duration': 16126800, 'prompt_eval_count': 10, 'prompt_eval_duration': 381314000, 'eval_count': 52, 'eval_duration': 993497000}
'''


def talk_to_ollama(txt) -> str:
    url = "http://192.168.10.12:11434/api/generate"
    params = {
        "model": "glm4:9b-chat-fp16",
        "prompt": txt,
        "stream": False
    }
    response = requests.post(url=url, json=params)
    return response.json()['response']
